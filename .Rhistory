rm_boundary_effect = T, bound_h = h[j],
sigma = sigma, sigma.bm = 1)
})
error_decomp_deg3 = data.frame(sup.error = as.vector(error_decomp),
p = rep(p, each = 4),
term = rep(c("sup.err", "bias", "eps", "Z"), length(p)))
error_decomp_deg3
##### Figure 2.7 #####
ggplot(error_decomp_deg3, aes(p, sup.error, lty = term, pch = term, col = term)) +
geom_point() +
geom_line() +
lims(x = c(0, 1000), y= c(0, 1.1)) +
labs(subtitle = "n = 600") + deriv_est_theme
##### source code #####
source("mean/functions_mean_derivative.R")
#### data from Figure 2.4 ####
load("mean/data/bandwidth_comparison_per_degree.RData")
p
##### source codes #####
source("mean/functions_mean_derivative.R")
#### simulation results #####
load("mean/data/bandwidth_comparison_per_degree.RData")
##### Bandwidth Comparison: Different degrees  #####
set.seed(264)
N = 1000
n = 600
p = c(65, 115, 175, 275, 400, 550, 750, 1000)  # setting in dis
sigma = 1
m = length(p)
H = sapply(p, function(x)(rev(seq(0.3, 4/x, -0.005))))
H
###### Degree = 2 ######
alpha = 2.1
H[[7]] = H[[7]][1:27] # not included in Figure 2.4
H[[8]] = H[[8]][1:27] # not included in Figure 2.4
H
H = sapply(p, function(x)(rev(seq(0.3, 4/x, -0.005))))
H[[7]] = H[[7]][1:27] # not included in Figure 2.4
H[[8]] = H[[8]][1:28] # not included in Figure 2.4
H
###### Degree = 2 ######
alpha = 2.1
deg = floor(alpha)
cl = makeCluster(detectCores( ) - 1);
plan(multisession);
erg_deg2 = lapply(1:m, function(j){
print(j)
return(h_est_BB_derivative(n, p[j], N, H[[j]], deg = deg, rm_boundary_effect = T,
sigma = sigma ))
})
stopCluster(cl);
erg_deg_2_df = data.frame(sup.err = unlist(erg_deg2), p =
factor(unlist(sapply(1:m, function(j){rep(p[j], length(H[[j]]))}))),
h = unlist(H)
)
Figure24a = ggplot(erg_deg_2_df, aes(x = h, y = sup.err, color = p, pch = p)) + # [erg_deg_2_df$p %in% c(115, 175, 275, 400, 550, 1000),]
geom_point() + labs(subtitle = "n = 600") + lims(y = c(0.5, 20)) + deriv_est_theme
Figure24a
Figure24a = ggplot(erg_deg_2_df |> filter(p %in% p[1:6]), aes(x = h, y = sup.err, color = p, pch = p)) + # [erg_deg_2_df$p %in% c(115, 175, 275, 400, 550, 1000),]
geom_point() + labs(subtitle = "n = 600") + lims(y = c(0.5, 20)) + deriv_est_theme
Figure24a = ggplot(erg_deg_2_df |> filter(p %in% c(65, 115, 175, 275, 400, 550)), aes(x = h, y = sup.err, color = p, pch = p)) + # [erg_deg_2_df$p %in% c(115, 175, 275, 400, 550, 1000),]
geom_point() + labs(subtitle = "n = 600") + lims(y = c(0.5, 20)) + deriv_est_theme
erg_deg_2_df
erg_deg_2_df |> filter(p %in% c(65, 115, 175, 275, 400, 550))
Figure24a = erg_deg_2_df |> filter(p %in% c(65, 115, 175, 275, 400, 550)) |>
ggplot(aes(x = h, y = sup.err, color = p, pch = p)) + # [erg_deg_2_df$p %in% c(115, 175, 275, 400, 550, 1000),]
geom_point() + labs(subtitle = "n = 600") + lims(y = c(0.5, 20)) + deriv_est_theme
Figure24a
ggsave("mean/grafics/derivative_bandwidth_comparison_quad.png", device = "png", width = 5, height = 3.8, units = "in")
###### Degree = 3 ######
set.seed(264) # same seed as for degree = 2
alpha = 3.1
deg = floor(alpha)
cl = makeCluster(detectCores( ) - 1);
plan(multisession);
erg_deg3 = lapply(1:m, function(j){
print(j)
return(h_est_BB_derivative(n, p[j], N, H[[j]], deg = deg, rm_boundary_effect = T,
sigma = sigma ))
})
stopCluster(cl);
erg_deg_3_df = data.frame(sup.err = unlist(erg_deg3), p =
factor(unlist(sapply(1:m, function(j){rep(p[j], length(H[[j]]))}))),
h = unlist(H)
)
# Local cubic estimator bandwidth comparison
Figure24b = erg_deg_3_df |> filter(p %in% c(65, 115, 175, 275, 400, 550)) |>
ggplot(aes(x = h, y = sup.err, color = p, pch = p)) +
geom_point() + lims(y = c(0.5, 20)) + labs(subtitle = "n = 600") + deriv_est_theme
Figure24b
ggsave("mean/grafics/derivative_bandwidth_comparison_cubic.png", device = "png", width = 5, height = 3.8, units = "in")
erg_deg_2_df |> group_by(p) |> slice_min(sup.err)
erg_deg_3_df |> group_by(p) |> slice_min(sup.err)
save.image("mean/data/bandwidth_comparison_per_degree.RData")
##### source code #####
source("mean/functions_mean_derivative.R")
#### data from Figure 2.4 ####
load("mean/data/bandwidth_comparison_per_degree.RData")
##### Error Decomp #####
erg_deg_3_df |>
group_by(p) |>
slice_min(sup.err) |>
pull(h)
which.h = sapply(erg_deg3, which.min)
h = numeric(m)
for(j in 1:m){
h[j] = H[[j]][which.h[j]]
}
h # optimal h in Table 2.1
set.seed(549)
error_decomp = sapply(1:m, function(j){
sampleAndDecompositionBB_derivative(n, p[j], h[j], N, deg = deg,
rm_boundary_effect = T, bound_h = h[j],
sigma = sigma, sigma.bm = 1)
})
error_decomp_deg3 = data.frame(sup.error = as.vector(error_decomp),
p = rep(p, each = 4),
term = rep(c("sup.err", "bias", "eps", "Z"), length(p)))
error_decomp_deg3
##### Figure 2.7 #####
ggplot(error_decomp_deg3, aes(p, sup.error, lty = term, pch = term, col = term)) +
geom_point() +
geom_line() +
lims(x = c(0, 1000), y= c(0, 1.1)) +
labs(subtitle = "n = 600") + deriv_est_theme
h # optimal h in Table 2.1
##### Figure 2.7 #####
ggplot(error_decomp_deg3, aes(p, sup.error, lty = term, pch = term, col = term)) +
geom_point() +
geom_line() +
lims(x = c(0, 1000), y= c(0, 1.2)) +
labs(subtitle = "n = 600") + deriv_est_theme
##### Figure 2.7 #####
ggplot(error_decomp_deg3, aes(p, sup.error, lty = term, pch = term, col = term)) +
geom_point() +
geom_line() +
lims(x = c(0, 1000), y= c(0, 1.4)) +
labs(subtitle = "n = 600") + deriv_est_theme
##### Figure 2.7 #####
ggplot(error_decomp_deg3, aes(p, sup.error, lty = term, pch = term, col = term)) +
geom_point() +
geom_line() +
lims(x = c(0, 1000), y= c(0, 1.3)) +
labs(subtitle = "n = 600") + deriv_est_theme
##### Figure 2.7 #####
ggplot(error_decomp_deg3, aes(p, sup.error, lty = term, pch = term, col = term)) +
geom_point() +
geom_line() +
lims(x = c(0, 1000), y= c(0, 1.35)) +
labs(subtitle = "n = 600") + deriv_est_theme
ggsave("Grafics/derivative_error_decomp_n600_cubic.png", device = "png", width = 5, height = 3.8, units = "in")
ggsave("mean/grafics/derivative_error_decomp_n600_cubic.png", device = "png", width = 5, height = 3.8, units = "in")
##### fixed p; growing n #####
set.seed(65)
p = 300
n_seq = c(10, 50, 100, 200, 400, 800)
h_seq = seq(0.3, 5/200, -0.005)
N = 1000
sigma = .5
# best bw
x = (0:p)/p
grid = seq(0, 1, 0.01)
multiple_weights = lapply(h_seq, function(h){
grid = grid[(grid > h) & (grid < (1-h))]
locPolWeights(x = x , bw = h, deg = 3,
xeval = grid, kernel = EpaK)$allWeig[,2,]
})
max_error_multiple_weights = lapply(n_seq, function(n){
max.errors = sapply(1:N, function(k){
Z_mean = bm(x, sigma = 1/sqrt(n))
e_mean = rnorm(p+1, mean=0, sd = sigma/sqrt(n))
Y = mu(x) + e_mean + Z_mean
sapply(1:length(h_seq), function(k){
grid = grid[(grid > h_seq[k]) & (grid < (1-h_seq[k]))]
max( abs( as.vector( multiple_weights[[k]] %*% Y) - mu_1(grid)))
})
}) |> rowMeans()
})
optimal_h = h_seq[max_error_multiple_weights |> sapply(which.min)]
res = sapply(1:length(n_seq), function(i){
sampleAndDecompositionBB_derivative(n_seq[i], p, optimal_h[i], N, deg = 3, sigma = sigma, rm_boundary_effect = T, bound_h = h_seq[i])
})
df.erg = data.frame(sup.error = as.vector(res),
n = rep(n_seq, each = 4),
term = rep(c("sup.err", "bias", "eps", "Z"), length(p)))
df.erg
ggplot(df.erg, aes(n, sup.error, lty = term, pch = term, col = term)) +
geom_point() +
geom_line() +
labs(subtitle = "p = 300") +
lims(y = c(0.1, 2.9)) +
deriv_est_theme
ggsave("Grafics/derivative_error_decomp_p300_cubic.png", device = "png",
width = 5, height = 3.8, units = "in")
ggsave("mean/grafics/derivative_error_decomp_p300_cubic.png", device = "png",
width = 5, height = 3.8, units = "in")
save.image("mean/data/error_decomposition.RData")
##### Simulation #####
n      = 100
p_seq  = c(200, 400, 800)
H      = lapply(p_seq, function(x)(rev(seq(0.4, 3/x, -0.02))))
cl = parallel::makeCluster(parallel::detectCores( ) - 1)
set.seed(22)
cl = parallel::makeCluster(parallel::detectCores( ) - 1)
future::plan(future::cluster)
future::plan(future::multisession)
parallel::stopCluster(cl)
cl = parallel::makeCluster(parallel::detectCores( ) - 1)
future::plan(future::multisession)
result = lapply(1:length(p_seq), FUN = function(i){
h_derivative(n = n, p = p_seq[i], p.eval = 100, N = 1000, h_seq = H[[i]], sigma = 1, m = 3)
})
#### sim ####
set.seed(22)
x = (1:p - 0.5)/p
x.eval = (1:p.eval - 0.5)/p.eval
#### results ####
load("mean/data/smoothness_comp_same_p_deg2.RData")
p.eval = 50
x = (1:p - 0.5)/p
#### sim ####
set.seed(22)
p.eval = 50
x = (1:p - 0.5)/p
#### results ####
load("mean/data/smoothness_comp_same_p_deg2.RData")
p = 800
n = c(10, 20, 40, 80, 160, 240, 480, 800, 1400)
#### sim ####
set.seed(22)
n = c(10, 20, 40, 80, 160, 240, 480, 800, 1400)
p = 800
p.eval = 50
x = (1:p - 0.5)/p
x.eval = (1:p.eval - 0.5)/p.eval
N = 1000
cl = makeCluster(detectCores(logical = F) - 1)
plan(multisession)
res = future_sapply(1:length(n), function(i){
w_smth = locPolWeights(x, x.eval, 2, h_smth[i], EpaK)$allWeig[,2,] |> as.matrix()
w_rgh  = locPolWeights(x, x.eval, 2,  h_rgh[i], EpaK)$allWeig[,2,] |> as.matrix()
max.errors = sapply(1:N, function(k){
Y_rough  = BM(n = 1, t = x,  sigma = 1/sqrt(n[i])) |> as.vector()
Y_smooth = biLocPol::z_2rv(n[i], p) |> colMeans()
e_rough  = max( abs( as.vector(w_rgh  %*% Y_rough) ))
e_smooth = max( abs( as.vector(w_smth %*% Y_smooth) ))
c(e_rough, e_smooth)
})
print(n[i])
rowMeans(max.errors)
}, future.seed = T)
#### results ####
load("mean/data/smoothness_comp_same_p_deg2.RData")
h_smth
h_rgh
#### sim ####
set.seed(22)
n = c(10, 20, 40, 80, 160, 240, 480, 800, 1400)
p = 800
p.eval = 50
x = (1:p - 0.5)/p
x.eval = (1:p.eval - 0.5)/p.eval
N = 1000
h_smth = c(0.425, 0.375, 0.375, 0.35, 0.325, 0.300, 0.275, 0.275, 0.250)
h_rgh  = c(0.450, 0.400, 0.425, 0.40, 0.350, 0.325, 0.325, 0.300, 0.300)
cl = makeCluster(detectCores(logical = F) - 1)
plan(multisession)
res = future_sapply(1:length(n), function(i){
w_smth = locPolWeights(x, x.eval, 2, h_smth[i], EpaK)$allWeig[,2,] |> as.matrix()
w_rgh  = locPolWeights(x, x.eval, 2,  h_rgh[i], EpaK)$allWeig[,2,] |> as.matrix()
max.errors = sapply(1:N, function(k){
Y_rough  = BM(n = 1, t = x,  sigma = 1/sqrt(n[i])) |> as.vector()
Y_smooth = biLocPol::z_2rv(n[i], p) |> colMeans()
e_rough  = max( abs( as.vector(w_rgh  %*% Y_rough) ))
e_smooth = max( abs( as.vector(w_smth %*% Y_smooth) ))
c(e_rough, e_smooth)
})
print(n[i])
rowMeans(max.errors)
}, future.seed = T)
library(biLocPol)
cl = makeCluster(detectCores(logical = F) - 1)
plan(multisession)
res = future_sapply(1:length(n), function(i){
w_smth = locPolWeights(x, x.eval, 2, h_smth[i], EpaK)$allWeig[,2,] |> as.matrix()
w_rgh  = locPolWeights(x, x.eval, 2,  h_rgh[i], EpaK)$allWeig[,2,] |> as.matrix()
max.errors = sapply(1:N, function(k){
Y_rough  = BM(n = 1, t = x,  sigma = 1/sqrt(n[i])) |> as.vector()
Y_smooth = biLocPol::z_2rv(n[i], p) |> colMeans()
e_rough  = max( abs( as.vector(w_rgh  %*% Y_rough) ))
e_smooth = max( abs( as.vector(w_smth %*% Y_smooth) ))
c(e_rough, e_smooth)
})
print(n[i])
rowMeans(max.errors)
}, future.seed = T)
stopCluster(cl);
round(res[1,] * sqrt(n)*h_rgh^0.5, 3)
round(res[2,] * sqrt(n), 3)
#### results ####
load("mean/data/smoothness_comp_same_p_deg2.RData")
load("C:/Users/mberger.PC12599/HESSENBOX/GitHub/from-dense-to-sparse-fda/mean/data/smoothness_comp_same_p.RData")
View(h_derivative_multiple_n)
##### Simulation #####
n      = 100
p_seq  = c(200, 400, 800)
H      = lapply(p_seq, function(x)(rev(seq(0.4, 3/x, -0.02))))
cl = parallel::makeCluster(parallel::detectCores( ) - 1)
future::plan(future::cluster)
parallel::stopCluster(cl)
h = seq(0.34, .1, by = -0.3)
h
h = seq(0.34, .1, by = -0.03)
h
#### sim ####
set.seed(22)
n = c(10, 20, 40, 80, 160, 240, 480, 800, 1600)
p = 800
p.eval = 50
x = (1:p - 0.5)/p
x.eval = (1:p.eval - 0.5)/p.eval
N = 1000
h = seq(0.34, .1, by = -0.03)
cl = makeCluster(detectCores(logical = F) - 1)
plan(multisession)
res = future_sapply(1:length(n), function(i){
w  = locPolWeights(x, x.eval, 2,  h[i], EpaK)$allWeig[,2,] |> as.matrix()
max.errors = sapply(1:N, function(k){
Y_rough  = biLocPol::BM(n = 1, t = x,  sigma = 1/sqrt(n[i])) |> as.vector()
Y_smooth = biLocPol::z_2rv(n[i], p) |> colMeans()
e_rough  = max( abs( as.vector(w  %*% Y_rough) ))
e_smooth = max( abs( as.vector(w %*% Y_smooth) ))
c(e_rough, e_smooth)
})
print(n[i])
rowMeans(max.errors)
}, future.seed = T)
stopCluster(cl);
round(res[1,] * sqrt(n)*h_rgh^0.5, 3)
round(res[1,] * sqrt(n)*h^0.5, 3)
round(res[2,] * sqrt(n), 3)
plot(res[1,] ~ n, type = "l", ylim = c(0, 3))
lines(res[2,] ~n, lty = 2)
lines(3.5/sqrt(n) ~n, col = "red")
lines(2.5*h_rgh^(-0.5)/sqrt(n) ~ n, col = "red")
plot(res[1,]*h_rgh^0.5/res[2,] ~ n, type = "l", ylim = c(0, 1.5))
save.image(("mean/data/rate_smooth_and_rough_processes.RData"))
mu_deriv_tibble |>
ggplot() +
geom_line(aes(x = x, y = val, col = fun, lty = fun)) +
scale_linetype_manual(
values = c(1, 2, 4),
breaks = c("mu'", "deg = 2", "deg = 3"),
labels = list(
"mu'" = expression(mu * "'"),
"deg = 2" = "Quad",
"deg = 3" = "Cubic"
)
) +
scale_colour_manual(
values = c("black", "green", "red"),
breaks = c("mu'", "deg = 2", "deg = 3"),
labels = list(
"mu'" = expression(mu * "'"),
"deg = 2" = "Quad",
"deg = 3" = "Cubic"
)
)  +
labs(title = "First derivative", y = NULL, x = NULL, colour = NULL, lty = NULL) +
deriv_est_theme
#### packages, code ####
library(ggplot2)
library(tidyverse)
library(biLocPol)
my_theme = theme_grey(base_size = 15) +
theme(plot.title = element_text(size = 14))
#  Figures 3.4, 3.5 and 3.6
source("cov/functions.r")
##### Figure 3.3 #####
bw_comparison_tbl = Reduce(rbind, bw_comparison) |>
as_tibble() |>
rename(n = V1, p = V2, h = V3, sup.err = V4) |>
mutate(p = as.factor(p), n = as.factor(n))
#### packages, code ####
library(ggplot2)
library(tidyverse)
library(biLocPol)
library(tictoc)
my_theme = theme_grey(base_size = 15) +
theme(plot.title = element_text(size = 14))
#  Figures 3.4, 3.5 and 3.6
source("cov/functions.r")
##### calculations #####
N = 1000
n.seq = c(50, 100, 200, 400)
p.seq = c(15, 25, 50, 75, 100)
p.eval = 100
H = lapply(1:length(p.seq), function(l){seq(1, 3/p.seq[l], -0.05)})
# Parameter OU Process
theta = 2; sigma = 3
# Standard deviation for additional errors
sd = 0.75
bw_comparison = list()
cl = parallel::makeCluster(parallel::detectCores( ) - 1)
future::plan(future::cluster)
##### Figure 3.3 #####
bw_comparison_tbl = Reduce(rbind, bw_comparison) |>
as_tibble() |>
rename(n = V1, p = V2, h = V3, sup.err = V4) |>
mutate(p = as.factor(p), n = as.factor(n))
library(tictoc)
##### calculations #####
N = 10
p.seq = c(20,50)
n. seq = c(100, 150)
n.seq = c(100, 150)
p.eval= 25
H = lapply(1:length(p.seq), function(l){seq(1, 3/p.seq[l], -0.05)})
# Parameter OU Process
theta = 2; sigma = 3
# Standard deviation for additional errors
sd = 0.75
bw_comparison = list()
cl = parallel::makeCluster(parallel::detectCores( ) - 1)
future::plan(future::cluster)
for(l in 1:length(p.seq)){
tic()
bw_comparison[[l]] = matrix(t(future_sapply(1:length(H[[l]]), FUN = function(k)
{
bandwidth_evaluation(H[[l]][k], p.seq[l], p.eval, n.seq, N,
cov_ou, list(theta = theta, sigma = sigma),
OU, list(alpha = theta, sigma = sigma, x0 = 0),
eps.arg = list(sd = sd))
},
future.seed = T)),ncol = 4)
cat("p =", p.seq[l], "done.")
toc()
}
library(future.apply)
for(l in 1:length(p.seq)){
tic()
bw_comparison[[l]] = matrix(t(future_sapply(1:length(H[[l]]), FUN = function(k)
{
bandwidth_evaluation(H[[l]][k], p.seq[l], p.eval, n.seq, N,
cov_ou, list(theta = theta, sigma = sigma),
OU, list(alpha = theta, sigma = sigma, x0 = 0),
eps.arg = list(sd = sd))
},
future.seed = T)),ncol = 4)
cat("p =", p.seq[l], "done.")
toc()
}
##### Figure 3.3 #####
bw_comparison_tbl = Reduce(rbind, bw_comparison) |>
as_tibble() |>
rename(n = V1, p = V2, h = V3, sup.err = V4) |>
mutate(p = as.factor(p), n = as.factor(n))
bw_comparison_tbl |>
filter(n == 400) |>
ggplot() +
geom_point(aes(x = h, y = sup.err, col = p, pch = p)) +
ylim(c(0.02, 0.52)) +
labs(title = "n = 400") +
my_theme
bw_comparison
Reduce(rbind, bw_comparison)
##### Figure 3.3 #####
bw_comparison_tbl = Reduce(rbind, bw_comparison) |>
as_tibble(.name_repair = "unique") |>
rename(n = V1, p = V2, h = V3, sup.err = V4) |>
mutate(p = as.factor(p), n = as.factor(n))
colnames(bw_comparison) = c("n", "p", "h", "sup.err")
warnings()
bw_comparison
colnames(bw_comparison) = c("n", "p", "h", "sup.err")
colnames(bw_comparison) = list(NULL, c("n", "p", "h", "sup.err"), NULL)
dim(bw_comparison)
is.arry(bw_comparison)
is.array(bw_comparison)
Reduce(rbind, bw_comparison)
Reduce(rbind, bw_comparison) |>
as_tibble()
Reduce(rbind, bw_comparison) |>
as_tibble() |>
rename(n = V1, p = V2, h = V3, sup.err = V4)
##### Figure 3.3 #####
bw_comparison_tbl = Reduce(rbind, bw_comparison) |>
as_tibble() |>
rename(n = V1, p = V2, h = V3, sup.err = V4) |>
mutate(p = as.factor(p), n = as.factor(n))
bw_comparison_tbl |>
filter(n == 400) |>
ggplot() +
geom_point(aes(x = h, y = sup.err, col = p, pch = p)) +
ylim(c(0.02, 0.52)) +
labs(title = "n = 400") +
my_theme
bw_comparison_tbl
bw_comparison_tbl |>
filter(n == 400) |>
ggplot() +
geom_point(aes(x = h, y = sup.err, col = p, pch = p)) +
# ylim(c(0.02, 0.52)) +
labs(title = "n = 400") +
my_theme
bw_comparison_tbl |>
#filter(n == 400) |>
ggplot() +
geom_point(aes(x = h, y = sup.err, col = p, pch = p)) +
# ylim(c(0.02, 0.52)) +
labs(title = "n = 400") +
my_theme
